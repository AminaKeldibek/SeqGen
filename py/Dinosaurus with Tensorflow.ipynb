{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/keldibek/anaconda2/envs/snakes36/lib/python3.6/site-packages/h5py/__init__.py:34: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from rnn_model import *\n",
    "from char_data_model import CharDataModel\n",
    "from sequence_generator import gen_sequence, decode_seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = CharDataModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n_a  = 64\n",
    "n_x = data.vocab_size\n",
    "n_y = n_x\n",
    "\n",
    "num_iters = 40000\n",
    "num_sequences = 5\n",
    "lr = 0.01\n",
    "\n",
    "a_0, params = init_params(n_a, n_x, n_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration  0 with loss  15.190118083507098\n",
      "joc\n",
      "baracesaurus\n",
      "el\n",
      "blaycaanalel\n",
      "saraldsataraura\n",
      "leocvvltatratan\n",
      "\n",
      "\n",
      "Iteration  2000 with loss  21.894699134820684\n",
      "ereonyysaurus\n",
      "n\n",
      "tomlochopeorus\n",
      "\n",
      "ktrchuyli\n",
      "sthanosaurus\n",
      "ur\n",
      "a\n",
      "\n",
      "\n",
      "Iteration  4000 with loss  10.362515079097296\n",
      "rihips\n",
      "ogrosaur\n",
      "paphosaurus\n",
      "uri\n",
      "egorausaurus\n",
      "sa\n",
      "thuorosaurus\n",
      "vo\n",
      "imasasrus\n",
      "us\n",
      "go\n",
      "\n",
      "\n",
      "Iteration  6000 with loss  31.818725880485278\n",
      "a\n",
      "hagte\n",
      "sikssyftes\n",
      "nill\n",
      "edrilus\n",
      "urhosau\n",
      "peplnis\n",
      "urus\n",
      "ya\n",
      "\n",
      "\n",
      "Iteration  8000 with loss  23.584448190176055\n",
      "chankops\n",
      "imenos\n",
      "eosachus\n",
      "goton\n",
      "jrakonoshus\n",
      "cho\n",
      "shlnodoravesior\n",
      "sinxodouodonodo\n",
      "\n",
      "\n",
      "Iteration  10000 with loss  29.02460816423141\n",
      "a\n",
      "jeoodia\n",
      "saurus\n",
      "\n",
      "nosacrucarosaur\n",
      "paeriaticosauru\n",
      "ccuanopen\n",
      "ozopo\n",
      "\n",
      "\n",
      "Iteration  12000 with loss  21.4200497132288\n",
      "osane\n",
      "eoc\n",
      "cenosus\n",
      "ehus\n",
      "ur\n",
      "pulostnosauraso\n",
      "ruxiovisaurapeo\n",
      "\n",
      "\n",
      "Iteration  14000 with loss  21.340617201368346\n",
      "huangcobarotoro\n",
      "coparys\n",
      "tosauru\n",
      "ornthytolortysi\n",
      "proelator\n",
      "odoet\n",
      "shictror\n",
      "icopto\n",
      "\n",
      "\n",
      "Iteration  16000 with loss  41.394451601774435\n",
      "qec\n",
      "ingurysaurus\n",
      "ol\n",
      "a\n",
      "mac\n",
      "lazonsaurtesaur\n",
      "\n",
      "\n",
      "Iteration  18000 with loss  18.08196308346242\n",
      "pratlososaurus\n",
      "\n",
      "troxnus\n",
      "os\n",
      "us\n",
      "s\n",
      "sileding\n",
      "a\n",
      "y\n",
      "s\n",
      "\n",
      "nrozyues\n",
      "us\n",
      "us\n",
      "\n",
      "siathosaurus\n",
      "s\n",
      "\n",
      "\n",
      "\n",
      "Iteration  20000 with loss  24.260604910149418\n",
      "zuhanantryxungu\n",
      "lemhe\n",
      "hytyramderchact\n",
      "pohynnangasauru\n",
      "prebunisaurus\n",
      "s\n",
      "\n",
      "\n",
      "Iteration  22000 with loss  14.587100315769074\n",
      "fexorecosaurus\n",
      "\n",
      "tabocedosaurus\n",
      "\n",
      "noc\n",
      "dlhgasaurus\n",
      "uch\n",
      "yipan\n",
      "is\n",
      "osauru\n",
      "\n",
      "\n",
      "Iteration  24000 with loss  13.985100665137558\n",
      "pryte\n",
      "eunovanophus\n",
      "ch\n",
      "ponhanathosauru\n",
      "chuenath\n",
      "myxaganasaurus\n",
      "\n",
      "\n",
      "\n",
      "Iteration  26000 with loss  32.68448681195978\n",
      "inoce\n",
      "bac\n",
      "a\n",
      "stamoceleschus\n",
      "\n",
      "tompinoni\n",
      "\n",
      "\n",
      "Iteration  28000 with loss  24.031263618862855\n",
      "brytasces\n",
      "totip\n",
      "jittarmisaurus\n",
      "\n",
      "catisaurus\n",
      "ipos\n",
      "grastisaurus\n",
      "yp\n",
      "dinnaf\n",
      "\n",
      "\n",
      "Iteration  30000 with loss  18.68384502300811\n",
      "dauricephmos\n",
      "uo\n",
      "qodeodon\n",
      "rocera\n",
      "piywe\n",
      "strithosaurus\n",
      "l\n",
      "iguce\n",
      "\n",
      "\n",
      "Iteration  32000 with loss  33.48402266152108\n",
      "bac\n",
      "a\n",
      "deyatata\n",
      "atam\n",
      "sanatatan\n",
      "a\n",
      "s\n",
      "a\n",
      "duanaua\n",
      "anatops\n",
      "\n",
      "\n",
      "Iteration  34000 with loss  27.711114804968204\n",
      "piahasaurus\n",
      "adi\n",
      "plc\n",
      "lergmosaurus\n",
      "os\n",
      "caradnis\n",
      "a\n",
      "am\n",
      "caiamanmus\n",
      "ian\n",
      "\n",
      "\n",
      "Iteration  36000 with loss  18.22443251477552\n",
      "sairoia\n",
      "yania\n",
      "i\n",
      "sekuoepia\n",
      "imasa\n",
      "stigaan\n",
      "onor\n",
      "ro\n",
      "orapalion\n",
      "anyas\n",
      "chaonyaurachace\n",
      "\n",
      "\n",
      "Iteration  38000 with loss  23.436941343423072\n",
      "jarirosaurus\n",
      "an\n",
      "protrus\n",
      "i\n",
      "tyrgudenminosas\n",
      "penosaurus\n",
      "nttr\n",
      "muohosaurus\n",
      "ete\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in np.arange(num_iters):\n",
    "    idx = i % data.train_size\n",
    "    X, Y = data.X[idx], data.Y[idx]\n",
    "    cache = rnn_fwd(X, a_0, params, data.vocab_size)\n",
    "    loss = calc_cost(cache['y_hat'], Y)\n",
    "    grads = rnn_bwd(Y, params, cache)\n",
    "    params = update_params(params, grads, lr)\n",
    "    \n",
    "    if i % 2000 == 0:\n",
    "        print (\"Iteration \", i, \"with loss \", loss)\n",
    "        for i in range(num_sequences):\n",
    "            sequence = gen_sequence(params, 15, data.char_to_idx[data.termin_elem],\n",
    "                                    a_0, np.zeros((n_x, 1)), 'random')\n",
    "            print (decode_seq(sequence, data.idx_to_char))\n",
    "        print ('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# SeqGen with Tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from rnn_model_tf import SeqGenModel\n",
    "from char_data_model import CharDataModel\n",
    "from sequence_generator import gen_sequence, decode_seq\n",
    "\n",
    "\n",
    "data = CharDataModel()\n",
    "n_a  = 64\n",
    "n_x = data.vocab_size\n",
    "n_y = n_x\n",
    "\n",
    "num_iters = 40000\n",
    "num_sequences = 5\n",
    "lr = 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.Graph().as_default() as graph:\n",
    "    seqgen_model = SeqGenModel(n_a, n_x, n_y)\n",
    "    init_op = tf.global_variables_initializer()\n",
    "#graph.finalize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'tensorflow' has no attribute 'softmax'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-ad352ea35c4a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m         \u001b[0midx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_size\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m         \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mY\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m         \u001b[0mcache\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mseqgen_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrnn_fwd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mseqgen_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcalc_cost\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcache\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'y_hat'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[0mtrain_op\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Projects/MusicGeneration/py/rnn_model_tf.py\u001b[0m in \u001b[0;36mrnn_fwd\u001b[0;34m(self, X, vocab_size)\u001b[0m\n\u001b[1;32m     68\u001b[0m                 \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m             \u001b[0ma\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_hat\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrnn_fwd_one_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     71\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m         \u001b[0mcache\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m'x'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'a'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'y_hat'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0my_hat\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Projects/MusicGeneration/py/rnn_model_tf.py\u001b[0m in \u001b[0;36mrnn_fwd_one_step\u001b[0;34m(self, a_prev, x)\u001b[0m\n\u001b[1;32m     46\u001b[0m         '''\n\u001b[1;32m     47\u001b[0m         \u001b[0ma_next\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtanh\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mWax\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mWaa\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ma_prev\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mba\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 48\u001b[0;31m         \u001b[0my_hat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msoftmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mWya\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ma_next\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mby\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0ma_next\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_hat\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: module 'tensorflow' has no attribute 'softmax'"
     ]
    }
   ],
   "source": [
    "with tf.Session(graph=graph) as session:\n",
    "    for i in np.arange(num_iters):\n",
    "        idx = i % data.train_size\n",
    "        X, Y = data.X[idx], data.Y[idx]\n",
    "        cache = seqgen_model.rnn_fwd(X, data.vocab_size)\n",
    "        loss = seqgen_model.calc_cost(cache['y_hat'], Y)\n",
    "        train_op = train(loss, lr)\n",
    "        _, loss = session.run([train_op, loss])\n",
    "    \n",
    "        '''if i % 2000 == 0:\n",
    "            print (\"Iteration \", i, \"with loss \", loss)\n",
    "            for i in range(num_sequences):\n",
    "                sequence = gen_sequence(params, 15, data.char_to_idx[data.termin_elem],\n",
    "                                        seqgen_model.a_0, np.zeros((n_x, 1)), 'random')\n",
    "                print (decode_seq(sequence, data.idx_to_char))\n",
    "            print ('\\n')'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
